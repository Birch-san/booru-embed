{
  // Use IntelliSense to learn about possible attributes.
  // Hover to view descriptions of existing attributes.
  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python: TSV Tokenizer",
      "type": "python",
      "request": "launch",
      "module": "script.tsv_tokenizer",
      "justMyCode": true
    },
    {
      "name": "Python: TSV Bucketer",
      "type": "python",
      "request": "launch",
      "module": "script.tsv_bucketer",
      "justMyCode": true
    },
    {
      "name": "Python: Make tokenizer",
      "type": "python",
      "request": "launch",
      "module": "script.make_tokenizer",
      "justMyCode": true
    },
    {
      "name": "Python: Read bucket",
      "type": "python",
      "request": "launch",
      "module": "script.read_bucket",
      "justMyCode": true
    },
    {
      "name": "Python: T5 MLM Flax",
      "type": "python",
      "request": "launch",
      "module": "script.run_t5_mlm_flax",
      "justMyCode": false,
      "args": [
        "--output_dir", "./out-t5-mlm-flax",
        "--model_type", "t5",
        "--config_name", "google/t5-v1_1-small",
        "--tokenizer_name", "google/t5-v1_1-small",
        "--use_fast_tokenizer", "True",
        // "--dataset_name", "oscar",
        "--dataset_name", "ArtifactAI/arxiv-math-instruct-50k",
        // "--dataset_config_name", "unshuffled_deduplicated_en",
        "--max_seq_length", "512",
        "--per_device_train_batch_size", "32",
        "--per_device_eval_batch_size", "32",
        "--adafactor",
        "--learning_rate", "0.005",
        "--weight_decay", "0.001",
        "--warmup_steps", "2000",
        "--overwrite_output_dir",
        "--logging_steps", "500",
        "--save_steps", "10000",
        "--eval_steps", "2500",
        // "--push_to_hub"
      ]
    },
    {
      "name": "Python: T5 MLM Torch",
      "type": "python",
      "request": "launch",
      "module": "script.run_t5_mlm_torch",
      "justMyCode": false,
      "args": [
        "--output_dir", "./out-t5-mlm-torch",
        "--model_type", "t5",
        "--config_name", "google/t5-v1_1-small",
        "--tokenizer_name", "google/t5-v1_1-small",
        "--use_fast_tokenizer", "True",
        // "--dataset_name", "oscar",
        "--dataset_name", "ArtifactAI/arxiv-math-instruct-50k",
        // "--dataset_config_name", "unshuffled_deduplicated_en",
        "--max_seq_length", "512",
        "--per_device_train_batch_size", "32",
        "--per_device_eval_batch_size", "32",
        "--adafactor",
        "--learning_rate", "0.005",
        "--weight_decay", "0.001",
        "--warmup_steps", "2000",
        "--overwrite_output_dir",
        "--logging_steps", "500",
        "--save_steps", "10000",
        "--eval_steps", "2500",
        // "--push_to_hub"
      ]
    },
    {
      "name": "Python: MLM Torch",
      "type": "python",
      "request": "launch",
      "module": "script.run_mlm_torch",
      "justMyCode": false,
      "args": [
        "--model_name_or_path", "roberta-base",
        "--dataset_name", "ArtifactAI/arxiv-math-instruct-50k",
        // "--dataset_name", "wikitext",
        // "--dataset_config_name", "wikitext-2-raw-v1",
        "--per_device_train_batch_size", "8",
        "--per_device_eval_batch_size", "8",
        "--do_train",
        // "--do_eval",
        "--output_dir", "./out-mlm-torch",
        "--overwrite_output_dir",
      ]
    },
    {
      "name": "Python: MLM Torch (no trainer)",
      "type": "python",
      "request": "launch",
      "module": "script.run_mlm_torch_no_trainer",
      "justMyCode": false,
      "args": [
        "--model_name_or_path", "roberta-base",
        "--dataset_name", "ArtifactAI/arxiv-math-instruct-50k",
        // "--dataset_name", "wikitext",
        // "--dataset_config_name", "wikitext-2-raw-v1",
        "--per_device_train_batch_size", "8",
        "--per_device_eval_batch_size", "8",
        "--output_dir", "./out-mlm-torch-no-trainer",
      ]
    },
    {
      "name": "Python: T5 MLM Torch 2",
      "type": "python",
      "request": "launch",
      "module": "script.run_t5_mlm_torch_2",
      "justMyCode": false,
      "args": [
        // "--model_name_or_path", "roberta-base",
        "--config_name", "google/t5-v1_1-small",
        "--tokenizer_name", "google/t5-v1_1-small",
        "--dataset_name", "ArtifactAI/arxiv-math-instruct-50k",
        // "--dataset_name", "wikitext",
        // "--dataset_config_name", "wikitext-2-raw-v1",
        "--per_device_train_batch_size", "8",
        "--per_device_eval_batch_size", "8",
        "--do_train",
        // "--do_eval",
        "--output_dir", "./out-t5-mlm-torch-2",
        "--overwrite_output_dir",
      ]
    },
    {
      "name": "Python: T5 MLM Torch Booru",
      "type": "python",
      "request": "launch",
      "module": "script.run_t5_mlm_torch_booru",
      "justMyCode": false,
      "args": [
        "--config_name", "./src/model",
        "--tokenizer_name", "google/t5-v1_1-small",
        "--per_device_train_batch_size", "256",
        "--per_device_eval_batch_size", "4",
        "--do_train",
        "--output_dir", "./out-t5-mlm-torch-booru",
        "--overwrite_output_dir",
        "--xformers",
        "--gradient_checkpointing",
        "--measure_flops",
        "--measure_memory",
        "--report_to", "wandb",
        "--optim", "adamw_8bit",
        "--log_every_n_steps", "5",
        "--collator_device", "cpu",
        "--pad_to_multiple", "8",
        // "--torch_compile",
        // "--torch_compile_mode", "reduce-overhead", // https://pytorch.org/docs/stable/generated/torch.compile.html
        // "--run_name", "batch_256_workers_1_cpu",
        // "--dataloader_num_workers", "1"
      ],
      "env": {
        "ACCELERATE_MIXED_PRECISION": "bf16",
      }
      // note: fastest seemed to be non-xformers, with single-precision. yikes.
      // batch 128 possible with xformers + mixed. 23371MiB / 24564MiB, reached iteration 255 and beyond
      // batch 128 also possible with sdp + mixed. 22921MiB / 24564MiB, though by iteration 150 was 24101MiB. made it to 255
      // need to compare iteration speed somehow. and determine which xformers kernel was used.
      // could try gradient checkpointing to trade iteration speed for batch size
      // could try more dataloader workers to alleviate iteration bottleneck
      // still got some loss masking to do
      // batch 128 mixed bf16 without xformers OOMed on iteration 49. depends on random luck I think.
      // gradient checkpointing reduces memory to 11403MiB, still batch size 128 xformers mixed, still >3it/s
    },
  ]
}